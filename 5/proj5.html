<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project 5: Exploring Diffusion Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
        }
        .description {
            margin-bottom: 40px;
            line-height: 1.6;
            text-align: center;
        }
        .image-grid {
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            justify-content: center;
        }
        .image-item {
            text-align: center;
            max-width: 30%; 
        }
        .large-image-item {
            text-align: center;
            width: 90%;
        }
        img {
            display: block;
            width: 100%;
            height: auto;
            border: 1px solid #ccc;
            border-radius: 5px;
        }
        .caption {
            margin-top: 10px;
            font-size: 14px;
            font-weight: bold;
            color: #333;
        }
        .section-title {
            font-size: 24px;
            margin-top: 40px;
            color: #333;
            text-align: center;
        }
        .centered-image {
            text-align: center;
            margin: 40px 0;
        }
    </style>
</head>
<body>
    <h1>Project 5: Exploring Diffusion Models</h1>

    <!-- Part 0: Setup -->
    <div class="section-title">Part 0: Setup</div>
    <div class="description">
        <p>This section introduces the DeepFloyd IF diffusion models used for generating images. 
            The models include a stage 1 U-Net for 64x64 images and a stage 2 U-Net for upscaling to 256x256. 
            Below are results of generated images using different iterations.</p>
    </div>
    
    <div class="image-item">
        <img src="./media/section0.png" alt="Output images after 20 iterations">
        <p class="caption">Output images after 20 iterations</p>
    </div>
    <div class="image-item">
        <img src="./media/section0_1.png" alt="Man Wearing Hat 40 Iterations">
        <p class="caption">Man Wearing Hat 40 Iterations</p>
    </div>

    <!-- Part 1: Sampling Loops -->
    <div class="section-title">Part 1: Sampling Loops</div>

    <!-- Part 1.1 -->
    <div class="section-title">Part 1.1: Implementing the Forward Process</div>
    <div class="description">
        <p>In this section, I progressively added noise to a test image at different timesteps. I did this by implementing a forward 
            pass with the formula x_t = sqrt(a_bar_t) * ùë•0 + sqrt(1‚àía_bar_t) * e, where e ~ N(0,1)
            The results below illustrate the effect of increasing noise levels.</p>
    </div>
    <div class="image-grid">
        <div class="image-item">
            <img src="./media/noise250.png" alt="Campanile at t=250">
            <p class="caption">Noised Image (t=250)</p>
        </div>
        <div class="image-item">
            <img src="./media/noise500.png" alt="Campanile at t=500">
            <p class="caption">Noised Image (t=500)</p>
        </div>
        <div class="image-item">
            <img src="./media/noise750.png" alt="Campanile at t=750">
            <p class="caption">Noised Image (t=750)</p>
        </div>
    </div>

    <!-- Part 1.2 -->
    <div class="section-title">Part 1.2: Classical Denoising</div>
    <div class="description">
        <p>In this section, I attempt to denoise the images using Gaussian blurring on various noise levels. 
            The results below show the limitations of this approach to denoising.</p>
    </div>
   
    <div class="image-item">
        <img src="./media/section1_2.png" alt="Gaussian Blurring (t=750)">
        <p class="caption">Gaussian Blurring at various noise levels.</p>
    </div>
    

    <!-- Part 1.3 -->
    <div class="section-title">Part 1.3: Implementing One-Step Denoising</div>
    <div class="description">
        <p>In this section, a pre-trained U-Net was used for one-step denoising. I estimate the noise in the noisy image
            by passing it through stage_1.unet and then remove the noise from the noisy image to obtain an estimate of the original image.
            Here are the results for different levels of noise.</p>
    </div>
    
     <div class="image-item">
            <img src="./media/section1_3.png" alt="One-Step Denoising (t=250)">
            <p class="caption">One-Step Denoising at various noise levels</p>
    </div>
        

    <!-- Part 1.4 -->
    <div class="section-title">Part 1.4: Implementing Iterative Denoising</div>
    <div class="description">
        <p>In this section, I implemented iterative denoising in which I use stridded timesteps to iterative denoise the image gradually.
            This has improved results over one-step denoising by gradually reducing noise over time.</p>
    </div>
    
    <div class="image-item">
        <img src="./media/section1_4.png" alt="Iterative Denoising (5th loop)">
        <p class="caption">Iterative Denoising compared to other methods</p>
    </div>
    <div class="image-item">
        <img src="./media/section1_4_2.png" alt="Iterative Denoising (15th loop)">
        <p class="caption">Iterative Denoising outputs at various timesteps</p>
    </div>
        
    <!-- Part 1.5 -->
    <div class="section-title">Part 1.5: Diffusion Model Sampling</div>
    <div class="description">
        <p>In this section, images were generated from pure random noise using iterative denoising by starting with a completely noisy image
            and then gradually removing the noise to generate a sampled image. Here are the sampled results.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_5.png" alt="Sample 1">
        <p class="caption">Sample images from Diffusion Model</p>
    </div>

        <!-- Part 1.6 -->
    <div class="section-title">Part 1.6: Classifier Free Guidance</div>
    <div class="description">
        <p>In this section, I implement Classifier Free Guidance, where images are generated using a combination of 
            conditional and unconditional sampling. To get an unconditional noise estimate, 
            I pass an empty prompt embedding to the diffusion model and pass in 'a high quality photo' 
            for the prompt embedding. By setting the guidance weight (gamma) to 7, 
            I can generate higher quality images.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_61.png" alt="Sample 1">
        <p class="caption">Sample images using classifer free guidance with gamma=7</p>
    </div>

        <!-- Part 1.7 -->
    <div class="section-title">Part 1.7: Image-to-image Translation</div>
    <div class="description">
        <p>In this section, I implemented Image-to-image translation. To do so I take an original image, 
            add noise to it, and then force it back onto the image manifold without any conditioning. 
            This generates an image that is similar to the test image.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_71.png" alt="Sample 1">
        <p class="caption">Image-to-Image Translation of Campanile test image.</p>
    </div>
    <div class="image-item">
        <img src="./media/section1_7.png" alt="Sample 1">
        <p class="caption">Image-to-Image Translation of LaFerrari and a goat.</p>
    </div>

        <!-- Part 1.7.1 -->
    <div class="section-title">Part 1.7.1: Editing Hand-Drawn and Web Images</div>
    <div class="description">
        <p>This section explores editing hand-drawn and web-sourced images. 
            The process involves feeding these nonrealistic images into the model to attempt to bring them into the
        natural image manifold.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_1_11.png" alt="Sample 1">
        <p class="caption">Image of cartoon monkey from the web.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_1_2.png" alt="Sample 1">
        <p class="caption">Hand drawn image of a character.</p>
    </div>

    <div class="image-grid">
        <div class="image-item">
            <img src="./media/section1_7_1_3.png" alt="Sample 1">
            <p class="caption">Hand drawn image of a banana.</p>
        </div>
        <div class="image-item">
            <img src="./media/section1_7_1_31.png" alt="Sample 1">
            <p class="caption">Image of a banana through the model.</p>
        </div>
    </div>

            <!-- Part 1.7 -->
    <div class="section-title">Part 1.7.2: Inpainting</div>
    <div class="description">
        <p>In this section, I implement Inpainting in which a mask a part of an image and then fill in the area within the
            mask using diffusion models. Below are results of inpainting applied to iconic landmarks like the Campanile, Eiffel Tower, 
            and Leaning Tower of Pisa.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_2_1.png" alt="Sample 1">
        <p class="caption">Campanile Inpainting results.</p>
    </div>
    
    <div class="image-item">
        <img src="./media/section1_7_2_2.png" alt="Sample 1">
        <p class="caption">Eiffel Tower Inpainting results.</p>
    </div>
    
    <div class="image-item">
        <img src="./media/section1_7_2_3.png" alt="Sample 1">
        <p class="caption">Leaning Tower of Pisa Inpainting results.</p>
    </div>

            <!-- Part 1.7 -->
    <div class="section-title">Part 1.7.3: Text-Conditioned Image-to-image Translation</div>
    <div class="description">
        <p>In this section, I implemented Text-conditioned image-to-image translation which combines the 
            original image with a text prompt to guide the diffusion process in order to incorporate control through language. 
            The results below demonstrate transformations applied to landmarks based on specific text descriptions.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_3_1.png" alt="Sample 1">
        <p class="caption">Campanile with a rocket ship prompt.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_3_2.png" alt="Sample 1">
        <p class="caption">Eiffel Tower with a pencil prompt.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_7_3_3.png" alt="Sample 1">
        <p class="caption">Leaning Tower of Pisa with a pencil prompt.</p>
    </div>

            <!-- Part 1.7 -->
    <div class="section-title">Part 1.8: Visual Anagrams</div>
    <div class="description">
        <p>In this section, I created Visual anagrams through the use of diffusion models to mix and overlay features from two 
            distinct prompts. To achieve this I denoise an image at a given timestep normally with a prompt
            to obtain a noise estimate. Then at the same time, I flip the image upside down and denoise with a different prompt 
            to get a second noise estimate. I flip this noise estimate back and average the two noise estimates before  
            performing a reverse diffusion step to create the anagram.</p>
    </div>

    <div class="image-grid">
        <div class="image-item">
            <img src="./media/section1_8_1.png" alt="Sample 1">
            <p class="caption">People Around Fire and Oil Painting of Old Man.</p>
        </div>
        <div class="image-item">
            <img src="./media/ana1.png" alt="Sample 1">
            <p class="caption">People Around Fire and Oil Painting of Old Man.</p>
        </div>
    </div>
        
    <div class="image-grid">
        <div class="image-item">
            <img src="./media/section1_8_2.png" alt="Sample 1">
            <p class="caption">An oil painting of a snowy mountain village and a photo of the amalfi cost</p>
        </div>
        <div class="image-item">
            <img src="./media/ana2.png" alt="Sample 1">
            <p class="caption">An oil painting of a snowy mountain village and a photo of the amalfi cost</p>
        </div>
    </div>

    <div class="image-grid">
        <div class="image-item">
            <img src="./media/section1_8_3.png" alt="Sample 1">
            <p class="caption">A photo of a dog and A man wearing a hat</p>
        </div>
        <div class="image-item">
            <img src="./media/ana3.png" alt="Sample 1">
            <p class="caption">A photo of a dog and A man wearing a hat</p>
        </div>
    </div>


    <!-- Part 1.10 -->
    <div class="section-title">Part 1.10: Hybrid Imagesg</div>
    <div class="description">
        <p>I created hybrid images by estimating the noise with two different text prompts, 
            and then combining low frequencies from one noise estimate with high frequencies of the other noise estimate.
        This generates a hybird image that looks different based on the distance it is viewed from.</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_10_1.png" alt="Sample 1">
        <p class="caption">An image of a skull (far), waterfalls (close). </p>
    </div>
    
    <div class="image-item">
        <img src="./media/section1_10_2.png" alt="Sample 1">
        <p class="caption">An image of a rocketship (far), amalfi coast (close)</p>
    </div>

    <div class="image-item">
        <img src="./media/section1_10_3.png" alt="Sample 1">
        <p class="caption">An image of snowy mountain village (far), amalfi coast (close)</p>
    </div>
        

    <!-- Part B -->
    <div class="section-title">Part B: Diffusion Models from Scratch</div>
    <div class="description">
        <p>In this section, I implemented and trained a diffusion model using the MNIST dataset. 
            This involved building and training a UNet for iterative denoising, exploring time-conditioning, and class-conditioning.</p>
    </div>
    
    <!-- Training Loss Curve -->
    <div class="section-title">Part 1: Training a Single-Step Denoising UNet</div>
    <div class="description">
        <p>In this part, I train a U-Net to denoise noisy MNIST images. To accomplish this, I add noise to the digits. Then,
        I train the U-Net on noisy and original image pairs in order for the net to learn how to denoise the iamges. For training,
        I set the noisy level to sigma=0.5. After training, I tested the performance on out of distribution images, meaning images
        with noise level other than sigma=0.5. </p>
    </div>
    
  
    <div class="image-item">
            <img src="./media/noisy_ex.png" alt="Noisy image examples">
            <p class="caption">Example of noisy images from MNIST</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch1.png" alt="Noisy image examples">
            <p class="caption">Unconditional U-Net after 1 Epoch</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch_5.png" alt="Noisy image examples">
            <p class="caption">Unconditional U-Net after 5 Epochs</p>
    </div>

    <div class="image-item">
            <img src="./media/loss1.png" alt="Noisy image examples">
            <p class="caption">Unconditional U-Net Loss Curve</p>
    </div>

    <div class="image-item">
            <img src="./media/OFD.png" alt="Noisy image examples">
            <p class="caption">Unconditional U-Net Out of Distribution Testing</p>
    </div>

    <!-- Class Conditioning -->
    <div class="section-title">Part 2: Training a Diffusion Model with Time Conditioning</div>
    <div class="description">
        <p>In this part, I make the U-Net predict the noise of an image instead of performing the denoising itself. 
          This is done since iterative denoising works better than one-step denoising. 
          To do this I add conditioning on the timestep of the diffusion process to the U-Net.
      Now the training workflow involves noising each image with a random timestep value 
          from 0 to 299, passing it through the U-Net to get the predicted noise, 
          and then calculating the loss between the predicted noise and actual noise added.
      To sample from the model, I utilize the iterative denoising algorithm where I start from an image of 
          pure noise, and then iteratively move towards the clean image. The pseudocode for the training and sampling algorithms along 
        with training results is shown below.</p>
    </div>

    <div class="image-grid">
        <div class="image-item">
            <img src="./media/alg_11.png" alt="Sample 1">
            <p class="caption">Training algorithm for time conditioned UNet</p>
        </div>
        <div class="image-item">
            <img src="./media/alg_12.png" alt="Sample 1">
            <p class="caption">Sampling algorithm for time conditioned UNet</p>
        </div>
    </div>
    
    <div class="image-item">
            <img src="./media/epoch_5_2.png" alt="Noisy image examples">
            <p class="caption">Time-Conditional U-Net after 5 Epochs</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch_20_2.png" alt="Noisy image examples">
            <p class="caption">Time-Conditional U-Net after 20 Epochs</p>
    </div>

    <div class="image-item">
            <img src="./media/loss2.png" alt="Noisy image examples">
            <p class="caption">Time-Conditional U-Net Loss Curve</p>
    </div>

    <div class="section-title">Part 2: Training a Diffusion Model with Class and Time Conditioning</div>
    <div class="description">
        <p> In this section, I add class-conditioning to the U-Net in order to improve the quality of the generated images.
          I now utilize the MNIST training data labels as a one-hot encoded vector and feed it as input into the U-Net as well to
          achieve class-conditioning. To sample, I pass in the one-hot encoded vector corresponding to the digit I want to generate. 
        Psuedocode for the training and sampling algorithms for the class conditioned model is shown people along with training results.</p>
    </div>

    <div class="image-grid">
        <div class="image-item">
            <img src="./media/alg_21.png" alt="Sample 1">
            <p class="caption">Training algorithm for class conditioned UNet</p>
        </div>
        <div class="image-item">
            <img src="./media/alg_22.png" alt="Sample 1">
            <p class="caption">Sampling algorithm for class conditioned UNet</p>
        </div>
    </div>

    <div class="image-item">
            <img src="./media/loss3.png" alt="Noisy image examples">
            <p class="caption">Class-Conditional U-Net Loss Curve</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch_1_3.png" alt="Noisy image examples">
            <p class="caption">Sampling results for the time-conditioned UNet for 5 epochs.</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch_5_3.png" alt="Noisy image examples">
            <p class="caption">Sampling results for the time-conditioned UNet for 5 epochs.</p>
    </div>
    <div class="image-item">
            <img src="./media/epoch_20_3.png" alt="Noisy image examples">
            <p class="caption">Sampling results for the time-conditioned UNet for 20 epochs.</p>
    </div>
  
</body>
</html>
